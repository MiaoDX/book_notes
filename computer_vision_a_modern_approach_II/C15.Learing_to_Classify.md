# C15.Learning to Classify

## 15.1 Classification, Error and Loss

### 15.1.1 Using Loss to Determine Decisions

The choice of classification rule must depend on the cost of making a mistake.

TP,
FP,
TN,
FN

The *risk* function of a particular classification strategy is the expected loss when using that strategy, as a function of the kind of item.

$R(s)=p(-1\rightarrow1|using s)L(-1\rightarrow1)+p(1\rightarrow-1|using s)L(-1\rightarrow1)$

L(i\rightarrowj) means the Loss of an item of type i is classified as an item of type j.


### 15.3.1 Manipulating Training Data to Improve Performance

How to get a large dataset:

* vary the scaling/cropping/rotation slightly to get many pictures
* we train on a subset of the examples, run the resulting classifier on the rest of the examples, and then insert the false positives and false negatives into the training set to retrain the classifier. This is because the false positives and false negatives are the cases that give the most information about errors in the configuration of the decision boundaries.  -- ** bootstrapping**

** hard negative mining**:

we have a moderate supply of positive examples, but an immense number of negative examples

selecting a set of negative examples, training with these, and then searching the rest of the negative examples to find ones that generate false positives -- these are hard negatives
